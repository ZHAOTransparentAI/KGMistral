{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001FD797A31A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: '2x90 min/week'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001FD797A31A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: '1,5x90 min/week'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001FD797A31A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: '6 weeks'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001FD797A31A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: '6 weeks'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001FD797A31A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: '6 weeks'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001FD797A31A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: '6 weeks'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001FD797A31A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: '3 hrs plus exercises'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001FD797A31A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: 'Exercises'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001FD797A31A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: 'Exercises'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001FD797A31A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: '5/9/2023'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001FD797A31A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: '5/9/2023'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001FD797A31A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: '6/16/2023'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001FD797A31A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: '6/16/2023'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001FD797A31A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: '6/16/2023'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001FD797A31A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: '6/16/2023'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001FD797A31A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: '7/15/2021'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001FD797A31A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: '4/20/2023'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001FD797A31A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: '4/20/2023'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001FD797A31A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\envs\\myenv\\Lib\\site-packages\\isodate\\isotime.py\", line 146, in parse_time\n",
      "    return time(int(hour), int(minute), int(second),\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: hour must be in 0..23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turtle file loaded successfully.\n",
      "Number of triples in the graph: 8166\n"
     ]
    }
   ],
   "source": [
    "import rdflib\n",
    "\n",
    "# Specify the path to your Turtle file\n",
    "turtle_file = 'output.ttl'\n",
    "\n",
    "# Create a new RDFLib Graph\n",
    "g = rdflib.Graph()\n",
    "\n",
    "# Attempt to load the Turtle file into the graph\n",
    "try:\n",
    "    g.parse(turtle_file, format=\"turtle\")\n",
    "    print(\"Turtle file loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Turtle file: {e}\")\n",
    "        \n",
    "# Print the number of triples in the graph\n",
    "print(f\"Number of triples in the graph: {len(g)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template 1: one given tail entity and one given relation - search head entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('http://demo.fiz-karlsruhe.de/matwerk/E1012754', 'mwo:hasWebsite', 'http://demo.fiz-karlsruhe.de/matwerk/E340077')\n"
     ]
    }
   ],
   "source": [
    "def search_head_entities(tail_entity_uri, relation_uri):\n",
    "    # Prepare the SPARQL query with dynamic relation and tail entity URIs\n",
    "    query = f\"\"\"\n",
    "    PREFIX dc: <http://purl.org/dc/elements/1.1/>\n",
    "    PREFIX dcterms: <http://purl.org/dc/terms/>\n",
    "    PREFIX default1: <https://nfdi.fiz-karlsruhe.de/ontology/>\n",
    "    PREFIX emmo: <http://emmo.info/emmo#>\n",
    "    PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
    "    PREFIX modsci: <https://w3id.org/skgo/modsci#>\n",
    "    PREFIX mwo: <http://purls.helmholtz-metadaten.de/mwo/>\n",
    "    PREFIX nfdicore: <http://nfdi.fiz-karlsruhe.de/ontology/>\n",
    "    PREFIX ns1: <https://w3id.org/scholarlydata/ontology/conference-ontology.owl#>\n",
    "    PREFIX ns2: <http://purl.obolibrary.org/obo/>\n",
    "    PREFIX ns3: <http://www.ebi.ac.uk/swo/>\n",
    "    PREFIX ns4: <http://www.geneontology.org/formats/oboInOwl#>\n",
    "    PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    PREFIX schema: <https://schema.org/>\n",
    "    PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "    PREFIX void: <http://rdfs.org/ns/void#>\n",
    "    PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "\n",
    "    SELECT ?headEntity\n",
    "    WHERE {{\n",
    "      ?headEntity {relation_uri} <{tail_entity_uri}> .\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the query and collect results as tuples\n",
    "    triples = []\n",
    "    try:\n",
    "        results = g.query(query)\n",
    "        for row in results:\n",
    "            head_entity = str(row.headEntity)\n",
    "            if head_entity.startswith(\"http\"):\n",
    "                head_entity = head_entity  # URIs are output directly without <>\n",
    "            else:\n",
    "                head_entity = f\":{head_entity}\"  # Prefixed names are prefixed with a colon\n",
    "\n",
    "            # Assuming relation_uri is a prefixed name, not a full URI, so it's output directly\n",
    "            tail_entity = tail_entity_uri  # URIs are output directly without <>\n",
    "\n",
    "            triple = f\"('{head_entity}', '{relation_uri}', '{tail_entity}')\"\n",
    "            triples.append(triple)\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing SPARQL query: {e}\")\n",
    "        return []\n",
    "\n",
    "    return triples\n",
    "\n",
    "# Example usage\n",
    "relation_uri = 'mwo:hasWebsite'\n",
    "tail_entity_uri = 'http://demo.fiz-karlsruhe.de/matwerk/E340077'\n",
    "triples = search_head_entities(tail_entity_uri,relation_uri)\n",
    "for triple in triples:\n",
    "    print(triple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template 2: one given head entity and one given relation - search tail entity, include literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('http://demo.fiz-karlsruhe.de/matwerk/E1004237', 'mwo:description', '\"GitHub, Inc., is an Internet hosting service for software development and version control using Git. It provides the distributed version control of Git plus access control, bug tracking, software feature requests, task management, continuous integration, and wikis for every project.\"')\n"
     ]
    }
   ],
   "source": [
    "import rdflib\n",
    "\n",
    "def search_tail_entities(head_entity_uri, relation_uri):\n",
    "    # Assuming 'g' is your RDFLib Graph object loaded with your Turtle data\n",
    "    global g\n",
    "\n",
    "    # Prepare the SPARQL query with dynamic head entity and relation URIs\n",
    "    query = f\"\"\"\n",
    "    PREFIX dc: <http://purl.org/dc/elements/1.1/>\n",
    "    PREFIX dcterms: <http://purl.org/dc/terms/>\n",
    "    PREFIX default1: <https://nfdi.fiz-karlsruhe.de/ontology/>\n",
    "    PREFIX emmo: <http://emmo.info/emmo#>\n",
    "    PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
    "    PREFIX modsci: <https://w3id.org/skgo/modsci#>\n",
    "    PREFIX mwo: <http://purls.helmholtz-metadaten.de/mwo/>\n",
    "    PREFIX nfdicore: <http://nfdi.fiz-karlsruhe.de/ontology/>\n",
    "    PREFIX ns1: <https://w3id.org/scholarlydata/ontology/conference-ontology.owl#>\n",
    "    PREFIX ns2: <http://purl.obolibrary.org/obo/>\n",
    "    PREFIX ns3: <http://www.ebi.ac.uk/swo/>\n",
    "    PREFIX ns4: <http://www.geneontology.org/formats/oboInOwl#>\n",
    "    PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    PREFIX schema: <https://schema.org/>\n",
    "    PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "    PREFIX void: <http://rdfs.org/ns/void#>\n",
    "    PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "\n",
    "    SELECT ?tailEntity\n",
    "    WHERE {{\n",
    "      <{head_entity_uri}> {relation_uri} ?tailEntity .\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the query and collect results as tuples\n",
    "    triples = []\n",
    "    try:\n",
    "        results = g.query(query)\n",
    "        for row in results:\n",
    "            # Construct a tuple for each triple\n",
    "            tail_entity = row.tailEntity.toPython() if isinstance(row.tailEntity, rdflib.URIRef) else f\"\\\"{row.tailEntity}\\\"\"\n",
    "            triple = (head_entity_uri, relation_uri, tail_entity)\n",
    "            triples.append(triple)\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing SPARQL query: {e}\")\n",
    "        return []\n",
    "\n",
    "    return triples\n",
    "\n",
    "# Example usage\n",
    "head_entity_uri = 'http://demo.fiz-karlsruhe.de/matwerk/E1004237'\n",
    "relation_uri = 'mwo:description'\n",
    "\n",
    "turtle_triples = search_tail_entities(head_entity_uri, relation_uri)\n",
    "for triple in turtle_triples:\n",
    "    print(triple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template 3:  one given head entity and one given relation, firstly find one tail entity, then find all head entities with same tail entity and same relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('http://demo.fiz-karlsruhe.de/matwerk/E1245566', 'mwo:hasAffiliation', 'http://demo.fiz-karlsruhe.de/matwerk/E1016')\n",
      "('http://demo.fiz-karlsruhe.de/matwerk/E16052', 'mwo:hasAffiliation', 'http://demo.fiz-karlsruhe.de/matwerk/E1016')\n",
      "('http://demo.fiz-karlsruhe.de/matwerk/E9779', 'mwo:hasAffiliation', 'http://demo.fiz-karlsruhe.de/matwerk/E1016')\n",
      "('http://demo.fiz-karlsruhe.de/matwerk/E14862', 'mwo:hasAffiliation', 'http://demo.fiz-karlsruhe.de/matwerk/E1016')\n"
     ]
    }
   ],
   "source": [
    "import rdflib\n",
    "\n",
    "def search_related_head_entities(initial_head_entity_uri, relation_uri):\n",
    "    # Define all the prefix namespaces\n",
    "    mwo_prefix = \"http://purls.helmholtz-metadaten.de/mwo/\"\n",
    "    dc_prefix = \"http://purl.org/dc/elements/1.1/\"\n",
    "    dcterms_prefix = \"http://purl.org/dc/terms/\"\n",
    "    default1_prefix = \"https://nfdi.fiz-karlsruhe.de/ontology/\"\n",
    "    emmo_prefix = \"http://emmo.info/emmo#\"\n",
    "    foaf_prefix = \"http://xmlns.com/foaf/0.1/\"\n",
    "    modsci_prefix = \"https://w3id.org/skgo/modsci#\"\n",
    "    nfdicore_prefix = \"http://nfdi.fiz-karlsruhe.de/ontology/\"\n",
    "    ns1_prefix = \"https://w3id.org/scholarlydata/ontology/conference-ontology.owl#\"\n",
    "    ns2_prefix = \"http://purl.obolibrary.org/obo/\"\n",
    "    ns3_prefix = \"http://www.ebi.ac.uk/swo/\"\n",
    "    ns4_prefix = \"http://www.geneontology.org/formats/oboInOwl#\"\n",
    "    owl_prefix = \"http://www.w3.org/2002/07/owl#\"\n",
    "    rdf_prefix = \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n",
    "    rdfs_prefix = \"http://www.w3.org/2000/01/rdf-schema#\"\n",
    "    schema_prefix = \"https://schema.org/\"\n",
    "    skos_prefix = \"http://www.w3.org/2004/02/skos/core#\"\n",
    "    void_prefix = \"http://rdfs.org/ns/void#\"\n",
    "    xsd_prefix = \"http://www.w3.org/2001/XMLSchema#\"\n",
    "\n",
    "    # Combine the two queries into one\n",
    "    combined_query = f\"\"\"\n",
    "    PREFIX mwo: <{mwo_prefix}>\n",
    "    PREFIX dc: <{dc_prefix}>\n",
    "    PREFIX dcterms: <{dcterms_prefix}>\n",
    "    PREFIX default1: <{default1_prefix}>\n",
    "    PREFIX emmo: <{emmo_prefix}>\n",
    "    PREFIX foaf: <{foaf_prefix}>\n",
    "    PREFIX modsci: <{modsci_prefix}>\n",
    "    PREFIX nfdicore: <{nfdicore_prefix}>\n",
    "    PREFIX ns1: <{ns1_prefix}>\n",
    "    PREFIX ns2: <{ns2_prefix}>\n",
    "    PREFIX ns3: <{ns3_prefix}>\n",
    "    PREFIX ns4: <{ns4_prefix}>\n",
    "    PREFIX owl: <{owl_prefix}>\n",
    "    PREFIX rdf: <{rdf_prefix}>\n",
    "    PREFIX rdfs: <{rdfs_prefix}>\n",
    "    PREFIX schema: <{schema_prefix}>\n",
    "    PREFIX skos: <{skos_prefix}>\n",
    "    PREFIX void: <{void_prefix}>\n",
    "    PREFIX xsd: <{xsd_prefix}>\n",
    "    \n",
    "    SELECT ?otherHeadEntity ?tailEntity\n",
    "    WHERE {{\n",
    "        {{\n",
    "            SELECT ?tailEntity\n",
    "            WHERE {{\n",
    "                <{initial_head_entity_uri}> {relation_uri} ?tailEntity .\n",
    "            }}\n",
    "        }}\n",
    "        ?otherHeadEntity {relation_uri} ?tailEntity .\n",
    "        FILTER (?otherHeadEntity != <{initial_head_entity_uri}>)  # Exclude the initial head entity\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        results = g.query(combined_query)\n",
    "        triples = [(str(row.otherHeadEntity), relation_uri, str(row.tailEntity)) for row in results]\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing the SPARQL query: {e}\")\n",
    "        return []\n",
    "\n",
    "    return triples\n",
    "\n",
    "# Example usage\n",
    "initial_head_entity_uri = 'http://demo.fiz-karlsruhe.de/matwerk/E15879'\n",
    "relation_uri = 'mwo:hasAffiliation'\n",
    "\n",
    "related_head_entities = search_related_head_entities(initial_head_entity_uri, relation_uri)\n",
    "for entity in related_head_entities:\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template 4:  one given tail entity and one given relation, firstly find one head entity, then find all tail entities with same tail entity and same relation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for all relevant triples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_all_possible_triples(entity_uri, relation_uri):\n",
    "    # Call the custom functions to find relevant triples\n",
    "    triples_query1 = search_head_entities(entity_uri, relation_uri)\n",
    "    triples_query2 = search_tail_entities(entity_uri, relation_uri)\n",
    "    triples_query3 = search_related_head_entities(entity_uri, relation_uri)\n",
    "    \n",
    "    # Combine the results from all three queries\n",
    "    all_triples = triples_query1 + triples_query2 + triples_query3\n",
    "def search_all_possible_triples(entity_uri, relation_uri):\n",
    "    if relation_uri == 'rdf:type':\n",
    "        # Call the custom functions to find relevant triples for rdf:type\n",
    "        triples_query1 = search_head_entities(entity_uri, relation_uri)\n",
    "        triples_query2 = search_tail_entities(entity_uri, relation_uri)\n",
    "        \n",
    "        # Combine the results from the two queries\n",
    "        all_triples = triples_query1 + triples_query2\n",
    "    else:\n",
    "        # Call the custom functions to find relevant triples\n",
    "        triples_query1 = search_head_entities(entity_uri, relation_uri)\n",
    "        triples_query2 = search_tail_entities(entity_uri, relation_uri)\n",
    "        triples_query3 = search_related_head_entities(entity_uri, relation_uri)\n",
    "        \n",
    "        # Combine the results from all three queries\n",
    "        all_triples = triples_query1 + triples_query2 + triples_query3\n",
    "\n",
    "    # Remove duplicates by converting the list of triples to a set and back to a list\n",
    "    all_triples = list(set(all_triples))\n",
    "    \n",
    "    return all_triples\n",
    "\n",
    "    # Remove duplicates by converting the list of triples to a set and back to a list\n",
    "    all_triples = list(set(all_triples))\n",
    "    \n",
    "    return all_triples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the file from Dataprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Named Entities</th>\n",
       "      <th>Predicate Verbs</th>\n",
       "      <th>Similar Relations</th>\n",
       "      <th>Relation URIs</th>\n",
       "      <th>Relation_uris_with_Namespace</th>\n",
       "      <th>Similar Entities</th>\n",
       "      <th>Entity URIs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who is working in the Computational Materials ...</td>\n",
       "      <td>the Computational Materials Science</td>\n",
       "      <td>work</td>\n",
       "      <td>['has work package', 'has expertise in', 'has ...</td>\n",
       "      <td>['https://nfdi.fiz-karlsruhe.de/ontology/examp...</td>\n",
       "      <td>['mwo:hasWorkPackage', 'mwo:hasExpertiseIn', '...</td>\n",
       "      <td>['Computational Material Science', 'computatio...</td>\n",
       "      <td>['http://demo.fiz-karlsruhe.de/matwerk/E67431'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the research projects associated to E...</td>\n",
       "      <td>EMMO</td>\n",
       "      <td>research project associate</td>\n",
       "      <td>['has related Project', 'related participant p...</td>\n",
       "      <td>['https://schema.org/dateCreated', 'http://nfd...</td>\n",
       "      <td>['nfdicore:relatedProject', 'mwo:relatedPartic...</td>\n",
       "      <td>['ruby', 'R. S. Elliott and E. B. Tadmor, \"Kno...</td>\n",
       "      <td>['http://demo.fiz-karlsruhe.de/matwerk/E837572...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who are the contributors of the data \"datasets\"?</td>\n",
       "      <td>datasets</td>\n",
       "      <td>contributor</td>\n",
       "      <td>['has contributor', 'related participant proje...</td>\n",
       "      <td>['http://www.geneontology.org/formats/oboInOwl...</td>\n",
       "      <td>['mwo:hasContributor', 'mwo:relatedParticipant...</td>\n",
       "      <td>['Image data', 'Framework for curation and dis...</td>\n",
       "      <td>['http://demo.fiz-karlsruhe.de/matwerk/E119683...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who is working with Researcher \"Ebrahim Norouz...</td>\n",
       "      <td>Ebrahim Norouzi</td>\n",
       "      <td>work</td>\n",
       "      <td>['has work package', 'has expertise in', 'has ...</td>\n",
       "      <td>['https://nfdi.fiz-karlsruhe.de/ontology/examp...</td>\n",
       "      <td>['mwo:hasWorkPackage', 'mwo:hasExpertiseIn', '...</td>\n",
       "      <td>['Dr. Amir Laadhar', 'Mirza Mohtashim Alam', '...</td>\n",
       "      <td>['http://demo.fiz-karlsruhe.de/matwerk/E10181'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who is the email address of \"ParaView\"?</td>\n",
       "      <td>ParaView</td>\n",
       "      <td>email address</td>\n",
       "      <td>['has email address ', 'has postal address', '...</td>\n",
       "      <td>['http://purl.obolibrary.org/obo/IAO_0000119',...</td>\n",
       "      <td>['mwo:emailAddress', 'mwo:hasPostalAddress', '...</td>\n",
       "      <td>['paraview', 'data portal', 'Standardised docu...</td>\n",
       "      <td>['http://demo.fiz-karlsruhe.de/matwerk/E123109...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  Who is working in the Computational Materials ...   \n",
       "1  What are the research projects associated to E...   \n",
       "2   Who are the contributors of the data \"datasets\"?   \n",
       "3  Who is working with Researcher \"Ebrahim Norouz...   \n",
       "4            Who is the email address of \"ParaView\"?   \n",
       "\n",
       "                        Named Entities              Predicate Verbs  \\\n",
       "0  the Computational Materials Science                         work   \n",
       "1                                 EMMO   research project associate   \n",
       "2                             datasets                  contributor   \n",
       "3                      Ebrahim Norouzi                         work   \n",
       "4                             ParaView                email address   \n",
       "\n",
       "                                   Similar Relations  \\\n",
       "0  ['has work package', 'has expertise in', 'has ...   \n",
       "1  ['has related Project', 'related participant p...   \n",
       "2  ['has contributor', 'related participant proje...   \n",
       "3  ['has work package', 'has expertise in', 'has ...   \n",
       "4  ['has email address ', 'has postal address', '...   \n",
       "\n",
       "                                       Relation URIs  \\\n",
       "0  ['https://nfdi.fiz-karlsruhe.de/ontology/examp...   \n",
       "1  ['https://schema.org/dateCreated', 'http://nfd...   \n",
       "2  ['http://www.geneontology.org/formats/oboInOwl...   \n",
       "3  ['https://nfdi.fiz-karlsruhe.de/ontology/examp...   \n",
       "4  ['http://purl.obolibrary.org/obo/IAO_0000119',...   \n",
       "\n",
       "                        Relation_uris_with_Namespace  \\\n",
       "0  ['mwo:hasWorkPackage', 'mwo:hasExpertiseIn', '...   \n",
       "1  ['nfdicore:relatedProject', 'mwo:relatedPartic...   \n",
       "2  ['mwo:hasContributor', 'mwo:relatedParticipant...   \n",
       "3  ['mwo:hasWorkPackage', 'mwo:hasExpertiseIn', '...   \n",
       "4  ['mwo:emailAddress', 'mwo:hasPostalAddress', '...   \n",
       "\n",
       "                                    Similar Entities  \\\n",
       "0  ['Computational Material Science', 'computatio...   \n",
       "1  ['ruby', 'R. S. Elliott and E. B. Tadmor, \"Kno...   \n",
       "2  ['Image data', 'Framework for curation and dis...   \n",
       "3  ['Dr. Amir Laadhar', 'Mirza Mohtashim Alam', '...   \n",
       "4  ['paraview', 'data portal', 'Standardised docu...   \n",
       "\n",
       "                                         Entity URIs  \n",
       "0  ['http://demo.fiz-karlsruhe.de/matwerk/E67431'...  \n",
       "1  ['http://demo.fiz-karlsruhe.de/matwerk/E837572...  \n",
       "2  ['http://demo.fiz-karlsruhe.de/matwerk/E119683...  \n",
       "3  ['http://demo.fiz-karlsruhe.de/matwerk/E10181'...  \n",
       "4  ['http://demo.fiz-karlsruhe.de/matwerk/E123109...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('relevant_entities_relations.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Named Entities</th>\n",
       "      <th>Predicate Verbs</th>\n",
       "      <th>Similar Relations</th>\n",
       "      <th>Relation URIs</th>\n",
       "      <th>Relation_uris_with_Namespace</th>\n",
       "      <th>Similar Entities</th>\n",
       "      <th>Entity URIs</th>\n",
       "      <th>Found Triples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who is working in the Computational Materials ...</td>\n",
       "      <td>the Computational Materials Science</td>\n",
       "      <td>work</td>\n",
       "      <td>['has work package', 'has expertise in', 'has ...</td>\n",
       "      <td>['https://nfdi.fiz-karlsruhe.de/ontology/examp...</td>\n",
       "      <td>['mwo:hasWorkPackage', 'mwo:hasExpertiseIn', '...</td>\n",
       "      <td>['Computational Material Science', 'computatio...</td>\n",
       "      <td>['http://demo.fiz-karlsruhe.de/matwerk/E67431'...</td>\n",
       "      <td>[\"('http://demo.fiz-karlsruhe.de/matwerk/E1031...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the research projects associated to E...</td>\n",
       "      <td>EMMO</td>\n",
       "      <td>research project associate</td>\n",
       "      <td>['has related Project', 'related participant p...</td>\n",
       "      <td>['https://schema.org/dateCreated', 'http://nfd...</td>\n",
       "      <td>['nfdicore:relatedProject', 'mwo:relatedPartic...</td>\n",
       "      <td>['ruby', 'R. S. Elliott and E. B. Tadmor, \"Kno...</td>\n",
       "      <td>['http://demo.fiz-karlsruhe.de/matwerk/E837572...</td>\n",
       "      <td>[('http://demo.fiz-karlsruhe.de/matwerk/E83757...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who are the contributors of the data \"datasets\"?</td>\n",
       "      <td>datasets</td>\n",
       "      <td>contributor</td>\n",
       "      <td>['has contributor', 'related participant proje...</td>\n",
       "      <td>['http://www.geneontology.org/formats/oboInOwl...</td>\n",
       "      <td>['mwo:hasContributor', 'mwo:relatedParticipant...</td>\n",
       "      <td>['Image data', 'Framework for curation and dis...</td>\n",
       "      <td>['http://demo.fiz-karlsruhe.de/matwerk/E119683...</td>\n",
       "      <td>[('http://demo.fiz-karlsruhe.de/matwerk/E11968...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who is working with Researcher \"Ebrahim Norouz...</td>\n",
       "      <td>Ebrahim Norouzi</td>\n",
       "      <td>work</td>\n",
       "      <td>['has work package', 'has expertise in', 'has ...</td>\n",
       "      <td>['https://nfdi.fiz-karlsruhe.de/ontology/examp...</td>\n",
       "      <td>['mwo:hasWorkPackage', 'mwo:hasExpertiseIn', '...</td>\n",
       "      <td>['Dr. Amir Laadhar', 'Mirza Mohtashim Alam', '...</td>\n",
       "      <td>['http://demo.fiz-karlsruhe.de/matwerk/E10181'...</td>\n",
       "      <td>[('http://demo.fiz-karlsruhe.de/matwerk/E29921...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who is the email address of \"ParaView\"?</td>\n",
       "      <td>ParaView</td>\n",
       "      <td>email address</td>\n",
       "      <td>['has email address ', 'has postal address', '...</td>\n",
       "      <td>['http://purl.obolibrary.org/obo/IAO_0000119',...</td>\n",
       "      <td>['mwo:emailAddress', 'mwo:hasPostalAddress', '...</td>\n",
       "      <td>['paraview', 'data portal', 'Standardised docu...</td>\n",
       "      <td>['http://demo.fiz-karlsruhe.de/matwerk/E123109...</td>\n",
       "      <td>[('http://demo.fiz-karlsruhe.de/matwerk/E12310...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  Who is working in the Computational Materials ...   \n",
       "1  What are the research projects associated to E...   \n",
       "2   Who are the contributors of the data \"datasets\"?   \n",
       "3  Who is working with Researcher \"Ebrahim Norouz...   \n",
       "4            Who is the email address of \"ParaView\"?   \n",
       "\n",
       "                        Named Entities              Predicate Verbs  \\\n",
       "0  the Computational Materials Science                         work   \n",
       "1                                 EMMO   research project associate   \n",
       "2                             datasets                  contributor   \n",
       "3                      Ebrahim Norouzi                         work   \n",
       "4                             ParaView                email address   \n",
       "\n",
       "                                   Similar Relations  \\\n",
       "0  ['has work package', 'has expertise in', 'has ...   \n",
       "1  ['has related Project', 'related participant p...   \n",
       "2  ['has contributor', 'related participant proje...   \n",
       "3  ['has work package', 'has expertise in', 'has ...   \n",
       "4  ['has email address ', 'has postal address', '...   \n",
       "\n",
       "                                       Relation URIs  \\\n",
       "0  ['https://nfdi.fiz-karlsruhe.de/ontology/examp...   \n",
       "1  ['https://schema.org/dateCreated', 'http://nfd...   \n",
       "2  ['http://www.geneontology.org/formats/oboInOwl...   \n",
       "3  ['https://nfdi.fiz-karlsruhe.de/ontology/examp...   \n",
       "4  ['http://purl.obolibrary.org/obo/IAO_0000119',...   \n",
       "\n",
       "                        Relation_uris_with_Namespace  \\\n",
       "0  ['mwo:hasWorkPackage', 'mwo:hasExpertiseIn', '...   \n",
       "1  ['nfdicore:relatedProject', 'mwo:relatedPartic...   \n",
       "2  ['mwo:hasContributor', 'mwo:relatedParticipant...   \n",
       "3  ['mwo:hasWorkPackage', 'mwo:hasExpertiseIn', '...   \n",
       "4  ['mwo:emailAddress', 'mwo:hasPostalAddress', '...   \n",
       "\n",
       "                                    Similar Entities  \\\n",
       "0  ['Computational Material Science', 'computatio...   \n",
       "1  ['ruby', 'R. S. Elliott and E. B. Tadmor, \"Kno...   \n",
       "2  ['Image data', 'Framework for curation and dis...   \n",
       "3  ['Dr. Amir Laadhar', 'Mirza Mohtashim Alam', '...   \n",
       "4  ['paraview', 'data portal', 'Standardised docu...   \n",
       "\n",
       "                                         Entity URIs  \\\n",
       "0  ['http://demo.fiz-karlsruhe.de/matwerk/E67431'...   \n",
       "1  ['http://demo.fiz-karlsruhe.de/matwerk/E837572...   \n",
       "2  ['http://demo.fiz-karlsruhe.de/matwerk/E119683...   \n",
       "3  ['http://demo.fiz-karlsruhe.de/matwerk/E10181'...   \n",
       "4  ['http://demo.fiz-karlsruhe.de/matwerk/E123109...   \n",
       "\n",
       "                                       Found Triples  \n",
       "0  [\"('http://demo.fiz-karlsruhe.de/matwerk/E1031...  \n",
       "1  [('http://demo.fiz-karlsruhe.de/matwerk/E83757...  \n",
       "2  [('http://demo.fiz-karlsruhe.de/matwerk/E11968...  \n",
       "3  [('http://demo.fiz-karlsruhe.de/matwerk/E29921...  \n",
       "4  [('http://demo.fiz-karlsruhe.de/matwerk/E12310...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "# Initialize a new column in the DataFrame to store the found triples for each row\n",
    "df['Found Triples'] = None\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    found_triples = []  # Initialize an empty list to store the found triples for the current row\n",
    "\n",
    "    # Parse the 'Entity URIs' and 'relationship_uris_withNS' columns from the current row\n",
    "    entities_uris = ast.literal_eval(row['Entity URIs'])\n",
    "    relationship_uris = ast.literal_eval(row['Relation_uris_with_Namespace'])\n",
    "\n",
    "    # Iterate through each combination of entity URI and relationship URI\n",
    "    for entity_uri in entities_uris:\n",
    "        for relation_uri in relationship_uris:\n",
    "            triples = search_all_possible_triples(entity_uri, relation_uri)\n",
    "            for triple in triples:\n",
    "                if triple not in found_triples:  # Check if the triple is not already in the list\n",
    "                    found_triples.append(triple)\n",
    "    \n",
    "    # Store the found triples for the current row in the new column\n",
    "    df.at[index, 'Found Triples'] =str(list(found_triples))  \n",
    "df.to_excel('sparql_output.xlsx', index=False)  # Save the updated DataFrame to a new Excel file\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select two coloumns\"Text\"\"Found Triples\" as new dataframe\n",
    "df_nwe= df[['Text', 'Found Triples']]\n",
    "df_nwe.to_excel('for_verbalisation.xlsx', index=False)  # Save the updated DataFrame to a new Excel file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iseenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
